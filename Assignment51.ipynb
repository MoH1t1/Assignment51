{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate.\n\nLinear Regression: Used for predicting a continuous target variable. It fits a linear relationship between the input features and the output.\nLogistic Regression: Used for binary classification problems. It estimates probabilities of outcomes, usually between 0 and 1, by applying the logistic (sigmoid) function.\n\nExample: Logistic regression would be more appropriate for predicting whether an email is spam or not (binary outcome), while linear regression is suited for\n         predicting house prices (continuous outcome).\n\n# Q2. What is the cost function used in logistic regression, and how is it optimized?\n\nThe cost function used in logistic regression is the logistic loss or cross-entropy loss. It measures the difference between the predicted probabilities and \nthe actual labels of the data. The goal is to minimize this cost function to ensure that the model's predictions are as accurate as possible.\n\nOptimization of the cost function is typically done using gradient descent. In this process, the model’s parameters (weights) are updated iteratively to reduce the\ncost. Gradient descent calculates the gradient (the derivative of the cost function) with respect to each parameter and adjusts the parameters in the direction\nthat reduces the cost. This process continues until the model converges to the optimal set of parameters that minimizes the cost function.       \n\n# Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.\n\nRegularization in logistic regression adds a penalty term to the cost function to prevent the model from becoming too complex and overfitting the training data.\nL1 regularization (Lasso): Can shrink some coefficients to zero, performing feature selection.\nL2 regularization (Ridge): Shrinks coefficients without eliminating them, reducing the influence of less important features.\nRegularization helps by constraining the model's complexity, improving generalization to unseen data.\n\n# Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?\n\nROC Curve: The Receiver Operating Characteristic (ROC) curve plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold values.\nIt helps evaluate the model’s ability to distinguish between classes. The area under the ROC curve (AUC) measures the model’s overall performance: a higher \nAUC indicates better performance.\n\n# Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?\n\nTechniques:\nCorrelation Matrix: Remove highly correlated features to avoid multicollinearity.\nRecursive Feature Elimination (RFE): Iteratively removes less important features based on model performance.\nL1 Regularization: Features with zero coefficients are removed.\nChi-Square Test: Measures the independence between features and the target variable.\n\nFeature selection improves model performance by reducing overfitting, improving interpretability, and decreasing computation time.\n\n# Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?\n\nStrategies:\nResampling: Use oversampling (SMOTE) or undersampling to balance the class distribution.\nClass Weights: Adjust the class weights in the logistic regression model to penalize misclassifications of the minority class more.\nSynthetic Data Generation: Generate synthetic examples for the minority class.\nEvaluation Metrics: Use metrics like Precision, Recall, F1-score, and AUC instead of accuracy to evaluate the model.\n\n# Q7. Can you discuss some common issues and challenges that may arise when implementing logistic regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?\n\nCommon Issues:\nMulticollinearity: Occurs when independent variables are highly correlated, making it difficult to determine the effect of each variable.\nSolution: Remove or combine correlated features, use principal component analysis (PCA), or apply regularization (L1 or L2).\nOutliers: Outliers can heavily influence the model.\nSolution: Identify and remove or handle outliers.\nImbalanced Data: Logistic regression may perform poorly with imbalanced classes.\nSolution: Use resampling, adjust class weights, or apply different evaluation metrics like AUC.\nOverfitting: Occurs if the model learns noise in the data.\nSolution: Apply regularization and cross-validation to improve generalization.\n\n                                                      ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}